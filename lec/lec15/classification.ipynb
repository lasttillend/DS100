{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression summary\n",
    "\n",
    "The logistic or sigmoid function can be written two equivalent ways:\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + \\exp(-t)} = \\frac{\\exp(t)}{1 + \\exp(t)} $$\n",
    "\n",
    "The logistic regression model assumes the following probabilities of $Y \\in \\{0, 1\\}$ given column vector $X$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(Y=1|X) &= \\sigma(X^T \\beta) &&= \\frac{1}{1 + \\exp(-X^T \\beta)} &= \\frac{\\exp(X^T\\beta)}{1 + \\exp(X^T\\beta)} \\\\[10pt]\n",
    "P(Y=0|X) &= \\sigma(-X^T \\beta) &&= \\frac{1}{1 + \\exp(X^T \\beta)}  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The loss most typically used to fit $\\beta$ is the log loss or cross-entropy loss, which is the negative log probability of the correct (observed) $Y$ value. This loss for true $Y \\in \\{0, 1\\}$ and predicted probability $\\hat Y \\in [0, 1]$ is often written:\n",
    "\n",
    "$$-Y \\log(\\hat Y) - (1-Y)\\log(1- \\hat Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    \"\"\"The logistic or sigmoid function, denoted σ(t).\n",
    "    \n",
    "    Note: This is actually a special case of what is generally \n",
    "          named the \"logistic\" function,\n",
    "          which allows for a different numerator and offset, \n",
    "          but lots of people call this the logistic function in practice.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def prediction(x, beta):\n",
    "    \"\"\"Prediction under the logistic model for features x and parameters b.\"\"\"\n",
    "    return sigma(x @ beta)\n",
    "\n",
    "def squared_loss(y, y_hat):\n",
    "    \"\"\"Squared loss applies to any true y and predicted y_hat.\"\"\"\n",
    "    return (y - y_hat) ** 2\n",
    "\n",
    "def log_loss(y, y_hat):\n",
    "    \"\"\"Log loss or cross-entropy loss, assuming y is in [0, 1].\"\"\"\n",
    "    assert y in [0, 1]\n",
    "    return -y * np.log(y_hat) - (1-y) * np.log(1-y_hat)\n",
    "\n",
    "def empirical_risk(true_ys, predicted_ys, loss):\n",
    "    \"\"\"The empirical risk is the average loss for a sample.\"\"\"\n",
    "    losses = [loss(y, y_hat) for y, y_hat in zip(true_ys, predicted_ys)]\n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Risk\n",
    "\n",
    "Filling in $\\hat Y = P(Y=1|X)$ and filling in the form of the model, we find different ways of expressing the same loss:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\beta) &= -Y \\log(\\hat Y) - (1-Y)\\log(1- \\hat Y) \\\\[10pt]\n",
    "         &= - Y \\log P(Y=1|X) - (1-Y) \\log P(Y=0|X)  \\\\[10pt]\n",
    "         &= - Y \\log \\frac{\\exp(X^T\\beta)}{1 + \\exp(X^T\\beta)} - (1-Y) \\log \\frac{1}{1 + \\exp(X^T\\beta)}  \\\\[10pt]\n",
    "         &= - Y (\\log(\\exp(X^T\\beta)) - \\log(1 + \\exp(X^T\\beta))) - (1-Y) (-\\log (1 + \\exp(X^T\\beta)))  \\\\[10pt]\n",
    "         &= - YX^T\\beta + Y \\log(1 + \\exp(X^T\\beta))) - Y \\log(1 + \\exp(X^T\\beta))) + \\log (1 + \\exp(X^T\\beta))  \\\\[10pt]\n",
    "         &= - YX^T\\beta + \\log (1 + \\exp(X^T\\beta)) \\\\[10pt]\n",
    "         &= -\\left(YX^T\\beta + \\log \\sigma(-X^T\\beta)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Where the last step follows from $\\log (1 + \\exp(X^T\\beta)) = -(- \\log (1 + \\exp(X^T\\beta))) = -\\log \\frac{1}{1 + \\exp(X^T\\beta)} = -\\log \\sigma(-X^T\\beta)$.\n",
    "\n",
    "The empirical risk (average loss across a sample) for a set of observations $(x_1, y_1) \\dots (x_n, y_n)$ is often written:\n",
    "\n",
    "$$R(\\beta, x, y) = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma(-x_i^T\\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gradient\n",
    "\n",
    "Using thelogistic regression model and log loss, find the gradient of the empirical risk.\n",
    "\n",
    "First, we compute the derivative of the sigmoid function since we'll use it in our gradient calculation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(t) &= \\frac{1}{1 + e^{-t}} \\\\[10pt]\n",
    "\\sigma'(t) &= \\frac{e^{-t}}{(1 + e^{-t})^2} \\\\[10pt]\n",
    "\\sigma'(t) &= \\frac{1}{1 + e^{-t}} \\cdot \\left(1 - \\frac{1}{1 + e^{-t}} \\right) \\\\[10pt]\n",
    "\\sigma'(t) &= \\sigma(t) (1 - \\sigma(t))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shorthand, we define $ \\sigma_i = \\sigma(-x_i^T \\beta) $. We will soon need the gradient of $ \\sigma_i $ with respect to the vector $ \\beta $ so we will derive it now using the chain rule. \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\beta} \\sigma_i\n",
    "&= \\nabla_{\\beta} \\sigma(-x_i^T \\beta) \\\\[10pt]\n",
    "&= \\sigma\\left(-x_i^T \\beta\\right) \\left(1 - \\sigma(-x_i^T \\beta)\\right)  \\nabla_{\\beta} \\left(-x_i^T \\beta\\right) \\\\[10pt]\n",
    "&= -\\sigma_i (1 - \\sigma_i) x_i \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we derive the gradient of the cross-entropy loss with respect to the model parameters $ \\boldsymbol{\\beta} $. We use the fact that $(1-\\sigma_i) = \\sigma(x_i^T\\beta)$, since $\\sigma(x^T\\beta) + \\sigma(-x^T\\beta) = 1$.\n",
    "\n",
    "\\begin{align*}\n",
    "R(\\beta, x, y) &= - \\frac{1}{n}\\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma_i \\right] \\\\[10pt]\n",
    "\\nabla_{\\beta} R(\\beta, x, y) &= - \\frac{1}{n}\\sum_{i=1}^n \\left( y_i x_i - \\frac{1}{\\sigma_i} \\sigma_i (1 - \\sigma_i) x_i \\right) \\\\[10pt]\n",
    "                              &= - \\frac{1}{n}\\sum_{i=1}^n \\left( y_i x_i - \\sigma(x_i^T\\beta) x_i \\right) \\\\[10pt]\n",
    "                              &= - \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\sigma(x_i^T\\beta)\\right) x_i  \\\\[10pt]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, beta0, gradient_function, max_iter=50000,  \n",
    "                     epsilon=1e-8, lr=0.1, clip=1):\n",
    "    \"\"\"Run gradient descent on a dataset (x, y) \n",
    "    with gradient clipping and learning rate decay.\"\"\"\n",
    "    beta = beta0\n",
    "    for t in range(1, max_iter):\n",
    "        grad = gradient_function(beta, x, y)\n",
    "        beta = beta - (lr/(1 + t/100)) * np.clip(grad, -clip, clip) \n",
    "        # Detect approximate convergence: small gradient\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            return beta\n",
    "    return beta\n",
    "\n",
    "def risk_gradient(beta, x, y):\n",
    "    \"\"\"Risk gradient for a whole dataset at once.\"\"\"\n",
    "    n = x.shape[0]\n",
    "#     print(\"n:\",n)\n",
    "    return -(1/n) * x @ (y - sigma(x.T @ beta)) \n",
    "\n",
    "def logistic_regression(x, y):\n",
    "    \"\"\"Train a logistic regression classifier using gradient descent.\"\"\"\n",
    "    beta0 = np.zeros(x.shape[0])\n",
    "    beta = gradient_descent(x, y, beta0, risk_gradient)\n",
    "    return beta    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                  17.990000\n",
       "mean texture                 10.380000\n",
       "mean perimeter              122.800000\n",
       "mean area                  1001.000000\n",
       "mean smoothness               0.118400\n",
       "mean compactness              0.277600\n",
       "mean concavity                0.300100\n",
       "mean concave points           0.147100\n",
       "mean symmetry                 0.241900\n",
       "mean fractal dimension        0.078710\n",
       "radius error                  1.095000\n",
       "texture error                 0.905300\n",
       "perimeter error               8.589000\n",
       "area error                  153.400000\n",
       "smoothness error              0.006399\n",
       "compactness error             0.049040\n",
       "concavity error               0.053730\n",
       "concave points error          0.015870\n",
       "symmetry error                0.030030\n",
       "fractal dimension error       0.006193\n",
       "worst radius                 25.380000\n",
       "worst texture                17.330000\n",
       "worst perimeter             184.600000\n",
       "worst area                 2019.000000\n",
       "worst smoothness              0.162200\n",
       "worst compactness             0.665600\n",
       "worst concavity               0.711900\n",
       "worst concave points          0.265400\n",
       "worst symmetry                0.460100\n",
       "worst fractal dimension       0.118900\n",
       "bias                          1.000000\n",
       "malignant                     1.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "cancer = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "cancer['bias'] = 1.0\n",
    "# Target data_dict['target'] = 0 is malignant; 1 is benign\n",
    "cancer['malignant'] = 1 - data_dict['target']\n",
    "cancer.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>bias</th>\n",
       "      <th>malignant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.372583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension     ...      worst perimeter  \\\n",
       "count     569.000000              569.000000     ...           569.000000   \n",
       "mean        0.181162                0.062798     ...           107.261213   \n",
       "std         0.027414                0.007060     ...            33.602542   \n",
       "min         0.106000                0.049960     ...            50.410000   \n",
       "25%         0.161900                0.057700     ...            84.110000   \n",
       "50%         0.179200                0.061540     ...            97.660000   \n",
       "75%         0.195700                0.066120     ...           125.400000   \n",
       "max         0.304000                0.097440     ...           251.200000   \n",
       "\n",
       "        worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       worst concave points  worst symmetry  worst fractal dimension   bias  \\\n",
       "count            569.000000      569.000000               569.000000  569.0   \n",
       "mean               0.114606        0.290076                 0.083946    1.0   \n",
       "std                0.065732        0.061867                 0.018061    0.0   \n",
       "min                0.000000        0.156500                 0.055040    1.0   \n",
       "25%                0.064930        0.250400                 0.071460    1.0   \n",
       "50%                0.099930        0.282200                 0.080040    1.0   \n",
       "75%                0.161400        0.317900                 0.092080    1.0   \n",
       "max                0.291000        0.663800                 0.207500    1.0   \n",
       "\n",
       "        malignant  \n",
       "count  569.000000  \n",
       "mean     0.372583  \n",
       "std      0.483918  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size:  426\n",
      "Test Data Size:  143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(cancer, test_size=0.25, random_state=100)\n",
    "print(\"Training Data Size: \", len(train))\n",
    "print(\"Test Data Size: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cede198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEBCAYAAAB7Wx7VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAGvNJREFUeJzt3W10XNV97/HvzMgeDbLk2vIQY2xD6pQ/4HTh5MZQCr4KxU0XJG1aQ50GHB4aQrNI3KZt2pVVWFFEr3tvmntZtA5kUSApjnESl9Ku28QQrmvXoeHJbkJoHNhJfYFYWA6y7AZpIglLM30xM/bMaEY6M5rRPOzf5w06e/Y5s7cO/mnPPnvOCaVSKUREpPWF690AERGZGwp8ERFPKPBFRDyhwBcR8YQCX0TEEwp8ERFPKPBFRDyhwBcR8YQCX0TEEwp8ERFPKPBFRDzRVuf3jwJrgQFgss5tERFpFhHgLGA/MB50p3oH/lrgyTq3QUSkWa0D/jVo5XoH/gDAiRMJksnZ3bWzu3sBQ0MjVWlUM1B/W5tP/fWpr1Cd/obDIRYt6oBMhgZV78CfBEgmU7MO/OxxfKL+tjaf+utTX6Gq/S1rKlwXbUVEPKHAFxHxhAJfRMQTCnwREU8o8EVEPKHAFxHxROBlmWbWBTwFvM8590rBa2uAB4Au4FvAR51zE1Vsp4iIzFKgwDezS4D7gfNKVNkO3OKce8bMHgQ+AnyhOk2USj198CiP7jvE0BvjhEOQTEF3V5QNPau4dPXSU/W+/M2X2Pf8EZIpCIegZ80yPvRr5586xrbHX2T85Ol1w5EQTLbIsulQCFJ16EsIKPW2ISA6P8IlK/Zww9rtLFlwjGMjS9i2fxP7DvXQs2ofN6zdTnzBMYZ+Fudvn70+r7ywPkDPqn3cePF2lnSc3uf7g+9h868dZM3ivyKc7CcZXk4i1st4+0aiYzvpGO0jnOxndOIsHnzmgzzx4rr890gsYdtzm4oe5/njf8DWb67m7fEnuOmSh+k+Y5DRybPYtv96vvH9y3jv27/NDWsfJtY2AD9dyY+PbZ5SPxkp3p7cdgIlXyvsQ7H3Llp/sr/s31E55Qz1s7igD3MllArwf7uZPQA8BHwZeHfuCN/MzgH2OOdWZbbXAX3OuV8J8P7nAi8PDY3M+osI8Xgng4PDszpGM5mpv08fPMpDj73EmxPJKa/Nbwtz41Xnc+nqpXz5my+x97tHptS54h3LeNvyn+OBr/+gLoHou55V+/j4untpn3f6NiljJ6Ps/uEVrD9vb+Dyzz95G0DpY9le2ttOl6eIMTr/emJvPkyI0eDvXXCcsYkou10ZbS1Rv1R7UsQY7tgKQGdi85TXyunDjPUD/o4qKR/u2FpR6IfDIbq7FwC8FXgl6H6BAj/LzF5hauBfCnzOOXd5ZvttwC7nXKlPA7nORYFfkZn6+yf3fpuhN0rfU6m7K8rnbruMWz67h2K/+nAIFnVGpz2G1M6Dv/MRzuwcnFI+mQwTCU/9I16q/PXhOEBZx0oRIVTkC5zlvne1yku2J7wCgEjy8Kz7UK365ZZPhldwfNHBKeUzqTTwq3FrhTD5n05DwNTf0DQyDZ+1eLyzKsdpFtP19/gMQX38jXHi8c6iYQ/p6Z+ZjiHFlZpaKad8yYJjRY8dDhX/p1WqvNRxptunWDBV8t7VKi/Vnkiyv2j5dPtUq8+l6pdbHkn2z2luVSPw+0nfpjNrKTB1jmAaGuGXb6b+Lu6afnS+uCvK4ODwqbn9QhrhV6ZwKubMzkE+vu5ezn/Li3lTCTOVD48tYGFs6vlNpsJEioRQqfJjI0tOHTfoPqVGo+W+d7XKS4+OlwPljfCr1efqjfCXc7yC3MoZ4Ze3X9l7FHDOvQqMmdllmaIPAY/N9rgyOxt6VjG/rfjpnd8WZkPPKiB9gbaYnjXL2NCzilCoZk1sSTes3Z43PwzQPm+cqy54oqxySM8h5xo7GeWxF99TVvm2/ZvYtn9T6X0m8svT8803kyJW3nsXHGdsosy2lqhfqj0pYiRivSRivUVfK6cPM9YP+DuqpDwR62UuVTzCN7NdwKedcweA64H7M0s3vwP8dZXaJxXKrsKZaZVOdjVOqVU6gFbplFDLqZjO9hHu2vuJolNAL/3kgqKrdLLlxVbpAEVX6Zx19vqiK1Amxn4pf5XOs+lVOnnvkbNKp/A4B0f+gEe+v5qXfnJB/iqdA+mVMv1v/OKplTKhtpUc/M/NU+rnrtLJbU/hKh2g6GuFfSj23kXrF6zSCfI7Kqc8kuxnspFX6dTQueiibUXU3/qKju0sujokFYoRTh2fUr/cj/q0ncPgwn+vapsbVaOd21qrRn8rvWirb9qKVKBjtC8v7IH0doqqfNQnvqU2DRevKfBFKhAusUIkxAmGO7YyGV5BihCT4RUMd2wl0XlXWeUsvH6OeyQ+qPcTr0SaUjK8vOjqkGR4OePtG4vOzZZbLlJtGuGLzCA6tpPFJ1azZGghi0+sJjq2s+TqkLledSFSDgW+yDSyF2cjycOESBFJHqYzsRmg6FSMRurSyDSlI5JR7KZXpS7Odoz2cXzRQQW8NBUFvghTl1meHsmPFq1f6qKtSCPTlI4I0yyzJFK0fjLztX6RZqLAF2G6EfukLs5Ky1Dgi1B6xJ7MXIzVxVlpBZrDFwESsd6it0rI3u9EAS+tQCN8EdJfftJIXlqdRvgiGRrJS6vTCF9ExBMKfBERTyjwRUQ8ocAXEfGEAl9ExBMKfBERTyjwRUQ8ocAXEfGEAl9ExBMKfBERTyjwRUQ8ocAXEfGEAl9ExBMKfBERTyjwRUQ8ocAXEfFEoAegmNl1wB3APOBu59w9Ba+/E7gPmA8cBjY55/6zym0VEZFZmHGEb2ZnA1uAy4E1wK1mdmFBtb8CPu2cuwhwwCer3VAREZmdIFM664E9zrnjzrkE8AhwbUGdCNCV+fkMyHkStIiINIQgUzrLgIGc7QHg4oI6fwQ8YWZ3Awngkuo0T0REqiVI4IeBVM52CEhmN8wsBjwIrHfOPWdmfwRsA94btBHd3QuCVp1WPN5ZleM0C/W3tfnUX5/6CvXrb5DA7wfW5WwvBY7kbL8dGHXOPZfZvg/483IaMTQ0QjKZmrniNOLxTgYHh2d1jGai/lYmOraTjtE+wsl+kuHlJGK9jLdvrEILq8un8+tTX6E6/Q2HQxUNlIPM4e8GrjSzuJmdAVwDPJ7z+n8AK8zMMtvvB/aX3RKRGouO7aQzsZlI8jAhUkSSh+lMbCY6trPeTROZEzMGvnPuNeB2YC/wPLAjM3Wzy8ze5Zw7AdwE7DSzF4DfBW6uYZtFKtIx2keoYD1BiFE6Rvvq1CKRuRVoHb5zbgewo6Ds6pyfHwMeq27TRKornOwvq1yk1eibtuKNZHh5WeUirUaBL95IxHpJEcsrSxEjEeutU4tE5pYCX7wx3r6R4Y6tTIZXkCLEZHgFwx1bG3KVjkgtBJrDF2kV4+0bFfDiLY3wRUQ8ocAXEfGEAl9ExBMKfBERTyjwRUQ8ocAXEfGEAl9ExBMKfBERTyjwRUQ8ocAXEfGEAl9ExBMKfBERTyjwRUQ8ocAXEfGEAl9aUnRsJ4tPrGbJ0EIWn1itB5WLoPvhSwuKju2kM7H51APLI8nDdCY2A+he+OI1jfCl5XSM9p0K+6wQo3SM9tWpRSKNQYEvLSec7C+rXMQXCnxpOcnw8rLKRXyhwJeWk4j1kiKWV5YiRiLWW6cWiTQGBb60nPH2jQx3bGUyvIIUISbDKxju2KoLtuI9rdKRljTevlEBL1JAI3wREU8o8EVEPKHAFxHxhAJfRMQTgS7amtl1wB3APOBu59w9Ba8bcB+wCDgK/I5z7kSV2yoiIrMw4wjfzM4GtgCXA2uAW83swpzXQ8D/Bf6Xc+4i4LvAp2rTXBERqVSQKZ31wB7n3HHnXAJ4BLg25/V3Agnn3OOZ7b8A7kFERBpKkCmdZcBAzvYAcHHO9tuAo2b2IPAO4EVgczmN6O5eUE71kuLxzqocp1mov63Np/761FeoX3+DBH4YSOVsh4BkwTHeDfx359wBM/tz4C7gpqCNGBoaIZlMzVxxGvF4J4ODw7M6RjNRf1ubT/31qa9Qnf6Gw6GKBspBpnT6gbNytpcCR3K2jwI/cs4dyGx/hfxPACIi0gCCBP5u4Eozi5vZGcA1wOM5rz8FxM3sosz2rwP/Vt1miojIbM0Y+M6514Dbgb3A88AO59xzZrbLzN7lnBsFfgu438wOAr8C/HEtGy2SpUcZigQXaB2+c24HsKOg7Oqcn59F0zgyx/QoQ5Hy6Ju20rT0KEOR8ijwpWnpUYYi5VHgS9PSowxFyqPAl6alRxmKlEeBL01LjzIUKY8ecShNTY8yFAlOI3wREU8o8EVEPKHAFxHxhAJfRMQTCnwREU8o8EVEPKHAFxHxhAJfRMQTCnwREU8o8EVEPKHAFxHxhAJfRMQTCnxpDj99WM+uFZkl3S1TGl50bCcc/30iqZ8BenatSKU0wpeG1zHaB5mwz9Kza0XKp8CXhqdn14pUhwJfGp6eXStSHQp8aXiJWC+Ezsgr07NrRcqnwJeGN96+EZb+jZ5dKzJLWqUjzWHh9Rx/8zfq3QqRpqYRvoiIJxT4IiKeUOCLiHgiUOCb2XVm9gMz+5GZfWyaeu81s5er1zwREamWGQPfzM4GtgCXA2uAW83swiL13gL8byBU7UaKiMjsBRnhrwf2OOeOO+cSwCPAtUXqPQDou+4iIg0qSOAvAwZytgeAvK84mtnvA98Bnqle00REpJqCrMMPA6mc7RCQzG6Y2duBa4ArKfhDEFR394JKdpsiHu+synGahfrb2nzqr099hfr1N0jg9wPrcraXAkdytn8bOAs4AMwHlpnZk8653H2mNTQ0QjKZmrniNOLxTgYHh2d1jGai/rY2n/rrU1+hOv0Nh0MVDZSDBP5u4DNmFgcSpEfzt2ZfdM71Ar0AZnYu8C/lhL2IiMyNGefwnXOvAbcDe4HngR3OuefMbJeZvavWDRQRkeoIdC8d59wOYEdB2dVF6r0CnFuNhomfomM76RjtI5zsJxleTiLWq5ukiVSJbp4mDSM6tpPOxGZCjAL5jzKED9evYSItQrdWkIbRMdp3Kuyz9ChDkepR4EvD0KMMRWpLgS8NQ48yFKktBb40jESslxSxvDI9ylCkehT40jDG2zcy3LFVjzIUqRGt0pGGMt6+UQEvUiMa4YuIeEKBLyLiCQW+iIgnFPhSF9GxnSw+sZolQwtZfGI10bGd9W6SSMvTRVuZc9PdQkEXbEVqRyN8mXO6hYJIfSjwZc7pFgoi9aHAlzmnWyiI1IcCX+acbqEgUh8KfJlzuoWCSH1olY7UhW6hIDL3NMIXEfGEAl9ExBMKfBERTyjwRUQ8ocCXmtI9c0Qah1bpSM3onjkijUUjfKkZ3TNHpLEo8KVmdM8ckcaiwJea0T1zRBqLAl9qRvfMEWksCnypGd0zR6SxBFqlY2bXAXcA84C7nXP3FLz+fqAPCAEvAzc7505Uua3ShHTPHJHGMeMI38zOBrYAlwNrgFvN7MKc17uALwDvdc5dBLwAfKYmrRURkYoFmdJZD+xxzh13ziWAR4Brc16fB3zMOfdaZvsFYGV1mykiIrMVZEpnGTCQsz0AXJzdcM4NAf8AYGYx4FPA1iq2UUREqiBI4IeBVM52CEgWVjKzhaSD/3vOuYfKaUR394JyqpcUj3dW5TjNoqH6+9OHYfB2mPgxtK2E+BZYeH1V36Kh+jsHfOqvT32F+vU3SOD3A+tytpcCR3IrmNlZwDeBPcAfltuIoaERksnUzBWnEY93Mjg4PKtjNJNG6m/hLRSYeJXUwEcYfmOsahdsG6m/c8Gn/vrUV6hOf8PhUEUD5SCBvxv4jJnFgQRwDXBr9kUziwD/BOx0zv2PslsgTW+6WyhohY5I45gx8J1zr5nZ7cBeYD7wgHPuOTPbBXwaWAG8E2gzs+zF3APOuVtq1WhpLLqFgkhzCLQO3zm3A9hRUHZ15scD6AtcXkuGlxNJHi5aLiKNQ0Ets6ZbKIg0BwW+lKXYA010CwWR5qAHoEhgMz3QRAEv0tg0wpfA9EATkeamwJfAtBpHpLkp8CUwPdBEpLkp8CUwrcYRaW4KfJmi2Eoc0ANNRJqdVulIniArcRTwIs1JI3zJo5U4Iq1LgS95tBJHpHUp8CWPVuKItC4FvuTRShyR1qXA95juiyPiF63S8ZTuiyPiH43wPaXVOCL+UeB7SqtxRPyjwPeUVuOI+EeB7ymtxhHxjwLfA1qNIyKgVTotT6txRCRLI/wWp9U4IpKlwG8hxaZutBpHRLI0pdMiSk3dpEKLCKWOT6mv1Tgi/tEIvwlFx3bCf5ybN5IvNXVDCq3GERFAgd8QSj1hqlTdzsRmmHiVEKlTI/lw8nDR+iFOaDWOiACa0qm7mVbRFCo1kk8RASan1E+Gl2s1jogACvy6m2kVTcdoH+FkP8nwchKx3mkutk6SIpZ3LE3diEguTenUWelVNOmRfiR5OG/qJhVaVLR+MjNVo6kbESlFI/w6S4aXEyk6/x4pPnWTipUcyWvqRkSmEyjwzew64A5gHnC3c+6egtfXAA8AXcC3gI865yaq3NaisitUGOpncWbaY7x946ny3OmQSsp/fOhvOGfeX7L4jEGO/yzOqyf/lJWrbj1df7KfwcQStj23iX2HeggBv3flC6w7+14WzP8Jx0aW8OX9mwh1fZCu5N/zGxd8kSULjnFsZAnb9m8CruXj6+6lfd74qT6NnYwSbRuHUJEOp07wf/Z+ghvWbs87zr5DS4A9c/Erbwgd7RGu+1Xj0tVLp7z29MGjfGX3DxkZnThV9+IL3sILh4YYemOc7q4oG3pWFd23lKcPHuXRfYcq3l+kEYRSqdS0FczsbOBfgf8GjANPAR90zv0gp873gVucc8+Y2YPAAefcFwK8/7nAy0NDIyST07ejmMILnpAe7Y7Ov57Ymw/PuvzlxG+yLPoo7W05YTwR5cj4Bt7a8Y959cdORvn8k7cBFA3w3T+8gvXn7Z1Snt2nMMBvWLudMzsHp/T59eE4H/7q/WX/rlpRJAS/+74L84L36YNH+dKuF5mYnP7/p/ltYW686vxAof30waM89NhLvDmRrGj/SsTjnQwODtfk2I3Gp75CdfobDofo7l4A8FbglaD7BRnhrwf2OOeOA5jZI8C1wJ2Z7XOAmHPumUz9vwX6gCCBPyulLnjG3vwSoYIVK5WUnxP7GpFwMq+8vW2cc8JfI0RB+bxxbli7/dTPha9ddcETU4+V2efDX72ffYd6pvSv2B+O9KcCAZhMwaP7DuWF7qP7Ds0Y9gBvTiSn7FvKo/sO5YV9ufuLNIoggb8MGMjZHgAunuH1sr7GmflLVb6h4hc8C8O70vJwKFlW+ZIFx4qWV7JP9g/A1KmbqX8YfHb8jXHi8c687Ur3na7ebPavVC2P3Wh86ivUr79BAj8M5A6ZQpA3vJ3p9RlVOqWzuMQFzxSRoiFebnkyFSZSJKhLlR8bWQJQdCpmpn2K2XeoRwE/g8Vd0byPx4u7ogwFDP3CfaerV+yYQfevhE/THD71Fao+pVPefgHq9ANn5WwvBY6U8XrNlHqIx+j8m6tS/uroBxibiOaVj01EeXX0A1PqZ6dbtu3fxNjJ6JTXHnvxPUXLNUVTuUgINvSsyivb0LOKtkixq9355reFp+xbyoaeVcxvy/+nUs7+Io0iSODvBq40s7iZnQFcAzyefdE59yowZmaXZYo+BDxW9ZYWkfsQD3LWnic67yq6Jr3c8s6V93FwZAvHEmeSTIU4ljiTgyNb6Fx53+n6qRCvj8T5/JO3se9QD9861MM/H76dN8aXkkyFeH04zj1P3sYPR+9k+3c/wevD8VPl2X2kfB3tkSkXbAEuXb2Um6++gAWxtry6V7xjGd1d6T+43V3Rsi64Xrp6KTdedX7F+4s0ihlX6cCpZZl/BswHHnDO/aWZ7QI+7Zw7YGYXAfeTXpb5HeBm51yQz9XnMotVOrn0sbC1qb+ty6e+QuOv0sE5twPYUVB2dc7P3yP/Qq6IiDQY3VpBRMQTCnwREU8o8EVEPKHAFxHxhAJfRMQTCnwREU/U+374EUivKa2Gah2nWai/rc2n/vrUV5h9f3P2j5SzX6AvXtXQ5cCT9WyAiEgTW0f69vWB1Dvwo8Ba0nfYLH7LShERKRQhfQ+z/aSfUxJIvQNfRETmiC7aioh4QoEvIuIJBb6IiCcU+CIinlDgi4h4QoEvIuIJBb6IiCfqfWuFWTOzvcCZwMlM0e85556tY5Nqwsy6gKeA9znnXjGz9cBdQAz4mnPujro2sMqK9PdLpL+ZnchU6XPO/UPdGlhFZtYLbMxsfsM596etfH5L9LeVz++dwLVACnjQOXdXvc5vU3/xysxCQD9wjnNuot7tqRUzu4T0M4PPB84DfgI4oAc4DHwDuNs5NycPj6+1wv5mAv/fgfc45wbq27rqyvzD7wOuIB0IjwMPAJ+lBc9vif5+HriT1jy/PcAW4N3APOAHwG8C/0Qdzm+zT+lY5r9PmNn3zOzjdW1N7XwE+BhwJLN9MfAj59zLmT9024HfrlfjaiCvv2Z2BrAS+KKZvWBmfWbW7P/vZg0Af+yce9M5dxJ4kfQf9VY9v8X6u5IWPb/OuX3AFZnzeCbpWZWfo07nt9mndBYB/wxsJv3X81/MzDnn/l99m1VdzrlbAMyyf99YRvofTtYAsHyOm1UzRfq7FNgD3Ab8FPg68GHSnwKamnPuYPZnM/sF0lMdW2nR81uiv+tIj4Bb7vwCOOdOmlkf8Eng76jjv9+mDnzn3NPA09ltM3sQuBpoqcAvIkz643BWCEjWqS0155z7/8BvZbfNbCtwAy0SCABmtpr0R/s/ASZIj/KzWu785vbXOedo8fPrnOs1s8+Snso5jzr9+23qj01mdrmZXZlTFOL0xdtW1k/6TnlZSzk93dNyzOwXzeyanKKWOs9mdhnpT6qfcs49RIuf38L+tvL5NbPzzWwNgHPuZ8CjpD/N1OX8NvUIn/Rc2J1m9sukp3RuBD5a3ybNiWcBM7O3AS8D1wFfrG+TaioE3G1me4AR4Fbgofo2qTrMbAXwj8AHnHN7MsUte35L9Ldlzy/w80CfmV1OelT/fuA+4HP1OL9NPcJ3zn2d9MfC7wL/BnwxM83T0pxzY8BNwN+Tvur/EvBIPdtUS865F4D/CXybdH+fd859pb6tqppPAu3AXWb2vJk9T/rc3kRrnt9i/f1lWvT8Oud2kZ9RTznnvkqdzm9TL8sUEZHgmnqELyIiwSnwRUQ8ocAXEfGEAl9ExBMKfBERTyjwRUQ8ocAXEfGEAl9ExBP/BeE58H+G2YVqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a271ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "radii = np.linspace(5, 30, 50)\n",
    "averages = [np.average(train[np.abs(train['mean radius']-r)<2]['malignant']) for r in radii]\n",
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(t):\n",
    "    return t[['bias', 'mean radius']].values.T\n",
    "    \n",
    "x_train, y_train = features(train), train['malignant'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-14.8970826 ,   1.01064211]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(fit_intercept=False, C=1e9, solver='lbfgs')\n",
    "model.fit(x_train.T, y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a42d208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');\n",
    "plt.plot(radii, sigma(model.coef_[0,0] + radii * model.coef_[0,1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = logistic_regression(x_train, y_train)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00645388, -0.0004522 ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_gradient(beta, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/426 (86.9%)\n"
     ]
    }
   ],
   "source": [
    "def print_ratio(n, d):\n",
    "    print('{}/{} ({:.1f}%)'.format(n, d, 100 * n/d))\n",
    "\n",
    "def accuracy(x, y, beta):\n",
    "    y_hat = np.round(sigma(x.T @ beta))\n",
    "    guesses = np.round(y_hat)\n",
    "    correct = y == guesses\n",
    "    print_ratio(sum(correct), len(guesses))\n",
    "\n",
    "accuracy(x_train, y_train, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/143 (60.8%)\n"
     ]
    }
   ],
   "source": [
    "x_test = features(test)\n",
    "y_test = test['malignant'].values\n",
    "print_ratio(sum(1-y_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/143 (90.9%)\n"
     ]
    }
   ],
   "source": [
    "accuracy(x_test, y_test, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n"
     ]
    }
   ],
   "source": [
    "def all_features(t):\n",
    "    return t.drop('malignant', axis=1).values.T\n",
    "\n",
    "def evaluate(beta, features):\n",
    "    print('Train:', end=' ')\n",
    "    accuracy(features(train), y_train, beta)\n",
    "    print('Test:', end=' ')\n",
    "    accuracy(features(test), y_test, beta)\n",
    "    \n",
    "beta = logistic_regression(all_features(train), y_train)\n",
    "evaluate(beta, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n",
    "As with linear regression, one common way of reducing the variance of the parameter estimator is to add a regularization term to the empirical risk objective. E.g.,\n",
    "\n",
    "\\begin{align*}\n",
    "R(\\beta, x, y, \\lambda) &= - \\frac{1}{n}\\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma(-x_i^T\\beta) \\right] + \\frac{1}{2} C \\sum_{j=1}^J \\beta_j^2 \\\\[10pt]\n",
    "\\nabla_{\\beta} R(\\beta, x, y, \\lambda) &=  - \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\sigma(x_i^T\\beta)\\right) x_i + C \\beta \\\\[10pt]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 0.0009765625\n",
      "sum(beta**2) =  181.05862166469308\n",
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.00390625\n",
      "sum(beta**2) =  153.17656348299207\n",
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.015625\n",
      "sum(beta**2) =  84.76727676186307\n",
      "Train: 401/426 (94.1%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 0.0625\n",
      "sum(beta**2) =  16.86808437919907\n",
      "Train: 396/426 (93.0%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.25\n",
      "sum(beta**2) =  1.159763781457247\n",
      "Train: 392/426 (92.0%)\n",
      "Test: 135/143 (94.4%)\n",
      "\n",
      "c = 1.0\n",
      "sum(beta**2) =  0.21983095016705734\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 4.0\n",
      "sum(beta**2) =  0.08526863018405177\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 16.0\n",
      "sum(beta**2) =  0.06240644243334323\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 64.0\n",
      "sum(beta**2) =  0.043885815938907356\n",
      "Train: 388/426 (91.1%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 256.0\n",
      "sum(beta**2) =  0.016109390288629684\n",
      "Train: 387/426 (90.8%)\n",
      "Test: 136/143 (95.1%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def regularized_logistic_regression(x, y, c):\n",
    "    \"\"\"Train a logistic regression classifier using gradient descent.\"\"\"\n",
    "\n",
    "    def l2_regularized_gradient(beta, x, y):\n",
    "        return risk_gradient(beta, x, y) + c * beta\n",
    "\n",
    "    beta0 = np.zeros(x.shape[0])\n",
    "    beta = gradient_descent(x, y, beta0, l2_regularized_gradient)\n",
    "    return beta    \n",
    "\n",
    "def search_for_c(features):\n",
    "    for c in 2.0 ** np.arange(-10, 10, 2):\n",
    "        print(\"c =\", c)\n",
    "        beta = regularized_logistic_regression(features(train), y_train, c)\n",
    "        print(\"sum(beta**2) = \", sum(beta**2))\n",
    "        evaluate(beta, features)\n",
    "        print()\n",
    "        \n",
    "search_for_c(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 0.0009765625\n",
      "sum(beta**2) =  35.8486017048037\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 0.00390625\n",
      "sum(beta**2) =  31.318273815172542\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 0.015625\n",
      "sum(beta**2) =  20.747956964144077\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 0.0625\n",
      "sum(beta**2) =  9.935156093274612\n",
      "Train: 421/426 (98.8%)\n",
      "Test: 141/143 (98.6%)\n",
      "\n",
      "c = 0.25\n",
      "sum(beta**2) =  4.279626098449614\n",
      "Train: 419/426 (98.4%)\n",
      "Test: 141/143 (98.6%)\n",
      "\n",
      "c = 1.0\n",
      "sum(beta**2) =  1.717220435886093\n",
      "Train: 414/426 (97.2%)\n",
      "Test: 140/143 (97.9%)\n",
      "\n",
      "c = 4.0\n",
      "sum(beta**2) =  0.6094833404635616\n",
      "Train: 412/426 (96.7%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 16.0\n",
      "sum(beta**2) =  0.17553643009069883\n",
      "Train: 405/426 (95.1%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 64.0\n",
      "sum(beta**2) =  0.03500234439319296\n",
      "Train: 403/426 (94.6%)\n",
      "Test: 136/143 (95.1%)\n",
      "\n",
      "c = 256.0\n",
      "sum(beta**2) =  0.0041317425176505785\n",
      "Train: 399/426 (93.7%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def inputs(t):\n",
    "    return t.drop('malignant', axis=1).values\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(inputs(train))\n",
    "\n",
    "def scaled_features(t):\n",
    "    return scaler.transform(inputs(t)).T\n",
    "\n",
    "search_for_c(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/143 (96.5%)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=4, solver='lbfgs')\n",
    "model.fit(scaled_features(train).T, y_train)\n",
    "y_hat = model.predict(scaled_features(test).T)\n",
    "print_ratio(sum(y_hat == y_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "\\begin{align*}\n",
    "P(Y=y|X) &= \\frac{\\exp(X^T\\beta_{y})}{\\sum_{z=0}^d \\exp(X^T\\beta_z)} \\\\[10pt]\n",
    "L(\\beta_0,\\dots,\\beta_d, x_i, y_i) &= - \\log \\frac{\\exp(x_i^T\\beta_{y_i})}{\\sum_{z=0}^d \\exp(x_i^T\\beta_z)} \\\\[10pt]\n",
    "\\frac{\\partial}{\\partial \\beta_w} L(\\beta_0,\\dots,\\beta_d, x_i, y_i) &= -\\left(1[w=y_i] - \\frac{\\exp(x_i^T\\beta_w)}{\\sum_{z=0}^d \\exp(x_i^T\\beta_z)}\\right) x_i  \\\\[10pt]\n",
    "1[w=y_i] &= \\begin{cases}\n",
    "1 & \\text{if}\\ w=y_i \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = sklearn.datasets.load_iris()\n",
    "x = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "y = data_dict['target']\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([50, 50, 50]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
